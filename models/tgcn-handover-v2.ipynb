{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'simulator_data_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# dataset_path = \"path_to_your_dataset.csv\"  # Replace with your actual dataset path\n",
    "data = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate distance (mock-up for illustration)\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return ((x2 - x1)**2 + (y2 - y1)**2)**0.5\n",
    "\n",
    "# Modify your function to process each timestamp as follows:\n",
    "def create_graph_data_for_timestamp(df, timestamp,  vehicle_ids, tower_ids):\n",
    "    timestamp_data = df[df['timestamp'] == timestamp]\n",
    "\n",
    "\n",
    "    # Map specific vehicle and tower IDs to consecutive indices\n",
    "    vehicle_mapping = {vid: i for i, vid in enumerate(vehicle_ids)}\n",
    "    tower_mapping = {tid: i + len(vehicle_ids) for i, tid in enumerate(tower_ids)}\n",
    "\n",
    "    node_features = []\n",
    "    edge_index = []\n",
    "    edge_features = []\n",
    "\n",
    "    # Process vehicle nodes\n",
    "    for vid in vehicle_ids:\n",
    "        vehicle_data = timestamp_data[timestamp_data['vehicleId'] == vid]\n",
    "        if not vehicle_data.empty:\n",
    "            avg_speed = vehicle_data['vehicleSpeed'].mean()\n",
    "            avg_dir = vehicle_data['vehicleDirection'].mean()\n",
    "            avg_pos_x = vehicle_data['vehiclePosX'].mean()\n",
    "            avg_pos_y = vehicle_data['vehiclePosY'].mean()\n",
    "            vehicle_features = [avg_speed, avg_dir, avg_pos_x, avg_pos_y]\n",
    "        else:\n",
    "            # Provide some default or mean features if the vehicle is not present\n",
    "            vehicle_features = [0, 0, 0, 0]  # Placeholder, adjust as needed\n",
    "        node_features.append(vehicle_features)\n",
    "\n",
    "    # Process tower nodes\n",
    "    for tid in tower_ids:\n",
    "        tower_data = timestamp_data[timestamp_data['masterId'] == tid]\n",
    "        if not tower_data.empty:\n",
    "            avg_pos_x = tower_data['masterPosX'].mean()\n",
    "            avg_pos_y = tower_data['masterPosY'].mean()\n",
    "            avg_load = tower_data['masterLoad'].mean()\n",
    "        else:\n",
    "            # Default or mean features for absent towers\n",
    "            avg_pos_x = avg_pos_y = avg_load = 0  # Placeholder\n",
    "        tower_features = [avg_pos_x, avg_pos_y, avg_load, 0.0]  # Last value is placeholder\n",
    "        node_features.append(tower_features)\n",
    "\n",
    "    # Add edges for existing pairs in this timestamp\n",
    "    for _, row in timestamp_data.iterrows():\n",
    "        if row['vehicleId'] in vehicle_mapping and row['masterId'] in tower_mapping:\n",
    "            vehicle_id = vehicle_mapping[row['vehicleId']]\n",
    "            tower_id = tower_mapping[row['masterId']]\n",
    "            edge_index.append([vehicle_id, tower_id])\n",
    "            edge_features.append([row['throughput'], row['masterRssi'], row['distance']])\n",
    "\n",
    "    node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_features_tensor = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "    return Data(x=node_features_tensor, edge_index=edge_index_tensor, edge_attr=edge_features_tensor)\n",
    "\n",
    "# Load your dataset\n",
    "# df = data\n",
    "# timestamps = sorted(df['timestamp'].unique())\n",
    "# graph_datasets = [create_graph_data_for_timestamp(df, ts) for ts in timestamps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class TemporalGNN(nn.Module):\n",
    "    def __init__(self, node_features_dim, edge_features_dim, memory_dim):\n",
    "        super(TemporalGNN, self).__init__()\n",
    "        self.conv = GCNConv(node_features_dim, memory_dim)\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(2 * memory_dim, 2 * memory_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * memory_dim, edge_features_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Generate node embeddings using GCN\n",
    "        node_embeddings = self.conv(x, edge_index)\n",
    "        # Combine embeddings of connected nodes to form edge features\n",
    "        src, dest = edge_index\n",
    "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dest]], dim=1)\n",
    "        # Predict edge features from combined node embeddings\n",
    "        predicted_edge_features = self.edge_predictor(edge_features)\n",
    "        return predicted_edge_features\n",
    "\n",
    "def predictive_loss(predicted_edge_features, predicted_edge_index, actual_edge_features, actual_edge_index):\n",
    "    # Convert edge indices to set of tuples for comparison\n",
    "    predicted_edges = set(map(tuple, predicted_edge_index.t().cpu().tolist()))\n",
    "    actual_edges = set(map(tuple, actual_edge_index.t().cpu().tolist()))\n",
    "\n",
    "    # Find common edges between predicted and actual\n",
    "    common_edges = predicted_edges.intersection(actual_edges)\n",
    "\n",
    "    # Extract features for common edges from predictions and actuals\n",
    "    pred_features = [predicted_edge_features[i] for i, edge in enumerate(predicted_edge_index.t().tolist()) if tuple(edge) in common_edges]\n",
    "    actual_features = [actual_edge_features[i] for i, edge in enumerate(actual_edge_index.t().tolist()) if tuple(edge) in common_edges]\n",
    "\n",
    "    # If no common edges, return zero loss with gradients\n",
    "    if not pred_features or not actual_features:\n",
    "        return torch.tensor(0.0, requires_grad=True).to(predicted_edge_features.device)\n",
    "\n",
    "    # Stack features and calculate MSE loss\n",
    "    pred_features = torch.stack(pred_features)\n",
    "    actual_features = torch.stack(actual_features)\n",
    "    return torch.nn.functional.mse_loss(pred_features, actual_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Completed training for Epoch 1 \n",
      "Starting Epoch 2\n",
      "Completed training for Epoch 2 \n",
      "Starting Epoch 3\n",
      "Completed training for Epoch 3 \n",
      "Starting Epoch 4\n",
      "Completed training for Epoch 4 \n",
      "Starting Epoch 5\n",
      "Completed training for Epoch 5 \n",
      "Starting Epoch 6\n",
      "Completed training for Epoch 6 \n",
      "Starting Epoch 7\n",
      "Completed training for Epoch 7 \n",
      "Starting Epoch 8\n",
      "Completed training for Epoch 8 \n",
      "Starting Epoch 9\n",
      "Completed training for Epoch 9 \n",
      "Starting Epoch 10\n",
      "Completed training for Epoch 10 \n",
      "Starting Epoch 11\n",
      "Completed training for Epoch 11 \n",
      "Starting Epoch 12\n",
      "Completed training for Epoch 12 \n",
      "Starting Epoch 13\n",
      "Completed training for Epoch 13 \n",
      "Starting Epoch 14\n",
      "Completed training for Epoch 14 \n",
      "Starting Epoch 15\n",
      "Completed training for Epoch 15 \n",
      "Starting Epoch 16\n",
      "Completed training for Epoch 16 \n",
      "Starting Epoch 17\n",
      "Completed training for Epoch 17 \n",
      "Starting Epoch 18\n",
      "Completed training for Epoch 18 \n",
      "Starting Epoch 19\n",
      "Completed training for Epoch 19 \n",
      "Starting Epoch 20\n",
      "Completed training for Epoch 20 \n",
      "Starting Epoch 21\n",
      "Completed training for Epoch 21 \n",
      "Starting Epoch 22\n",
      "Completed training for Epoch 22 \n",
      "Starting Epoch 23\n",
      "Completed training for Epoch 23 \n",
      "Starting Epoch 24\n",
      "Completed training for Epoch 24 \n",
      "Starting Epoch 25\n",
      "Completed training for Epoch 25 \n",
      "Starting Epoch 26\n",
      "Completed training for Epoch 26 \n",
      "Starting Epoch 27\n",
      "Completed training for Epoch 27 \n",
      "Starting Epoch 28\n",
      "Completed training for Epoch 28 \n",
      "Starting Epoch 29\n",
      "Completed training for Epoch 29 \n",
      "Starting Epoch 30\n",
      "Completed training for Epoch 30 \n",
      "Starting Epoch 31\n",
      "Completed training for Epoch 31 \n",
      "Starting Epoch 32\n",
      "Completed training for Epoch 32 \n",
      "Starting Epoch 33\n",
      "Completed training for Epoch 33 \n",
      "Starting Epoch 34\n",
      "Completed training for Epoch 34 \n",
      "Starting Epoch 35\n",
      "Completed training for Epoch 35 \n",
      "Starting Epoch 36\n",
      "Completed training for Epoch 36 \n",
      "Starting Epoch 37\n",
      "Completed training for Epoch 37 \n",
      "Starting Epoch 38\n",
      "Completed training for Epoch 38 \n",
      "Starting Epoch 39\n",
      "Completed training for Epoch 39 \n",
      "Starting Epoch 40\n",
      "Completed training for Epoch 40 \n",
      "Starting Epoch 41\n",
      "Completed training for Epoch 41 \n",
      "Starting Epoch 42\n",
      "Completed training for Epoch 42 \n",
      "Starting Epoch 43\n",
      "Completed training for Epoch 43 \n",
      "Starting Epoch 44\n",
      "Completed training for Epoch 44 \n",
      "Starting Epoch 45\n",
      "Completed training for Epoch 45 \n",
      "Starting Epoch 46\n",
      "Completed training for Epoch 46 \n",
      "Starting Epoch 47\n",
      "Completed training for Epoch 47 \n",
      "Starting Epoch 48\n",
      "Completed training for Epoch 48 \n",
      "Starting Epoch 49\n",
      "Completed training for Epoch 49 \n",
      "Starting Epoch 50\n",
      "Completed training for Epoch 50 \n",
      "Starting Epoch 51\n",
      "Completed training for Epoch 51 \n",
      "Starting Epoch 52\n",
      "Completed training for Epoch 52 \n",
      "Starting Epoch 53\n",
      "Completed training for Epoch 53 \n",
      "Starting Epoch 54\n",
      "Completed training for Epoch 54 \n",
      "Starting Epoch 55\n",
      "Completed training for Epoch 55 \n",
      "Starting Epoch 56\n",
      "Completed training for Epoch 56 \n",
      "Starting Epoch 57\n",
      "Completed training for Epoch 57 \n",
      "Starting Epoch 58\n",
      "Completed training for Epoch 58 \n",
      "Starting Epoch 59\n",
      "Completed training for Epoch 59 \n",
      "Starting Epoch 60\n",
      "Completed training for Epoch 60 \n",
      "Starting Epoch 61\n",
      "Completed training for Epoch 61 \n",
      "Starting Epoch 62\n",
      "Completed training for Epoch 62 \n",
      "Starting Epoch 63\n",
      "Completed training for Epoch 63 \n",
      "Starting Epoch 64\n",
      "Completed training for Epoch 64 \n",
      "Starting Epoch 65\n",
      "Completed training for Epoch 65 \n",
      "Starting Epoch 66\n",
      "Completed training for Epoch 66 \n",
      "Starting Epoch 67\n",
      "Completed training for Epoch 67 \n",
      "Starting Epoch 68\n",
      "Completed training for Epoch 68 \n",
      "Starting Epoch 69\n",
      "Completed training for Epoch 69 \n",
      "Starting Epoch 70\n",
      "Completed training for Epoch 70 \n",
      "Starting Epoch 71\n",
      "Completed training for Epoch 71 \n",
      "Starting Epoch 72\n",
      "Completed training for Epoch 72 \n",
      "Starting Epoch 73\n",
      "Completed training for Epoch 73 \n",
      "Starting Epoch 74\n",
      "Completed training for Epoch 74 \n",
      "Starting Epoch 75\n",
      "Completed training for Epoch 75 \n",
      "Starting Epoch 76\n",
      "Completed training for Epoch 76 \n",
      "Starting Epoch 77\n",
      "Completed training for Epoch 77 \n",
      "Starting Epoch 78\n",
      "Completed training for Epoch 78 \n",
      "Starting Epoch 79\n",
      "Completed training for Epoch 79 \n",
      "Starting Epoch 80\n",
      "Completed training for Epoch 80 \n",
      "Starting Epoch 81\n",
      "Completed training for Epoch 81 \n",
      "Starting Epoch 82\n",
      "Completed training for Epoch 82 \n",
      "Starting Epoch 83\n",
      "Completed training for Epoch 83 \n",
      "Starting Epoch 84\n",
      "Completed training for Epoch 84 \n",
      "Starting Epoch 85\n",
      "Completed training for Epoch 85 \n",
      "Starting Epoch 86\n",
      "Completed training for Epoch 86 \n",
      "Starting Epoch 87\n",
      "Completed training for Epoch 87 \n",
      "Starting Epoch 88\n",
      "Completed training for Epoch 88 \n",
      "Starting Epoch 89\n",
      "Completed training for Epoch 89 \n",
      "Starting Epoch 90\n",
      "Completed training for Epoch 90 \n",
      "Starting Epoch 91\n",
      "Completed training for Epoch 91 \n",
      "Starting Epoch 92\n",
      "Completed training for Epoch 92 \n",
      "Starting Epoch 93\n",
      "Completed training for Epoch 93 \n",
      "Starting Epoch 94\n",
      "Completed training for Epoch 94 \n",
      "Starting Epoch 95\n",
      "Completed training for Epoch 95 \n",
      "Starting Epoch 96\n",
      "Completed training for Epoch 96 \n",
      "Starting Epoch 97\n",
      "Completed training for Epoch 97 \n",
      "Starting Epoch 98\n",
      "Completed training for Epoch 98 \n",
      "Starting Epoch 99\n",
      "Completed training for Epoch 99 \n",
      "Starting Epoch 100\n",
      "Completed training for Epoch 100 \n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "df = data\n",
    "# Convert timestamps to numeric and sort\n",
    "df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Define training and testing ranges\n",
    "train_end_time = 8  # Define the timestamp up to which data should be used for training\n",
    "test_start_time = 8.5  # Define the timestamp from which data should be used for testing\n",
    "\n",
    "# Select train and test data based on timestamps\n",
    "train_data = df[df['timestamp'] <= train_end_time]\n",
    "test_data = df[df['timestamp'] == test_start_time]\n",
    "\n",
    "# Example of creating graph data for each set\n",
    "vehicle_ids = sorted(df['vehicleId'].unique())  # Ensure you have a list of all vehicle IDs\n",
    "tower_ids = sorted(df['towerId'].unique())  # Ensure you have a list of all tower IDs\n",
    "\n",
    "# Function to create graph data (assuming it's already defined)\n",
    "train_graphs = [create_graph_data_for_timestamp(train_data, ts, vehicle_ids, tower_ids) for ts in train_data['timestamp'].unique()]\n",
    "test_graphs = [create_graph_data_for_timestamp(test_data, ts, vehicle_ids, tower_ids) for ts in test_data['timestamp'].unique()]\n",
    "\n",
    "\n",
    "model = TemporalGNN(node_features_dim=4, edge_features_dim=3, memory_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epoch = 100\n",
    "# Training loop\n",
    "for epoch in range(num_epoch):  # Number of training epochs\n",
    "    print(f\"Starting Epoch {epoch+1}\")\n",
    "    for graph_data in train_graphs:\n",
    "        optimizer.zero_grad()\n",
    "        predicted_edge_features = model(graph_data.x, graph_data.edge_index)\n",
    "        loss = predictive_loss(predicted_edge_features, graph_data.edge_index, graph_data.edge_attr, graph_data.edge_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Completed training for Epoch {epoch+1} \")\n",
    "\n",
    "# # Testing loop\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for graph_data in test_graphs:\n",
    "#         predicted_edge_features = model(graph_data.x, graph_data.edge_index)\n",
    "#         print(f\"Predicted edge features for timestamp {test_start_time}: {predicted_edge_features}\")\n",
    "\n",
    "\n",
    "# model = TemporalGNN(node_features_dim=4, edge_features_dim=3, memory_dim=64)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Define the training range for timestamps 2 to 6 (indices 1 to 5 in zero-indexed Python lists)\n",
    "# train_start_index = 5\n",
    "# train_end_index = 7\n",
    "\n",
    "# # Define the index for prediction which is timestamp 7 (index 6)\n",
    "# prediction_index = 6\n",
    "\n",
    "# # Training the model\n",
    "# for epoch in range(50):  # Number of epochs\n",
    "#     total_loss = 0\n",
    "#     count = 0  # Count of batches that actually contribute to the loss\n",
    "#     for i in range(train_start_index, train_end_index + 1):  # Loop from timestamp 2 to 6\n",
    "#         optimizer.zero_grad()\n",
    "#         current_data = graph_datasets[i]\n",
    "#         next_data = graph_datasets[i + 1]  # Get the next timestamp data\n",
    "#         predicted_edge_features = model(current_data.x, current_data.edge_index)\n",
    "#         loss = predictive_loss(predicted_edge_features, current_data.edge_index, next_data.edge_attr, next_data.edge_index)\n",
    "#         if loss.requires_grad:  # Only backpropagate if the loss is not a placeholder\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#             count += 1\n",
    "#     if count > 0:\n",
    "#         print(f\"Epoch {epoch}, Avg Loss: {total_loss / count}\")\n",
    "#     else:\n",
    "#         print(f\"Epoch {epoch}, No valid training data\")\n",
    "\n",
    "# # Prediction\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_data = graph_datasets[prediction_index]  # Using the data of the timestamp 7 for prediction\n",
    "#     predicted_edge_features = model(test_data.x, test_data.edge_index)\n",
    "#     print(f\"Predicted edge features for timestamp {prediction_index+1}:\", predicted_edge_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_complete_bipartite_edge_index(vehicle_ids, tower_ids, vehicle_mapping, tower_mapping):\n",
    "    # Generate all possible combinations of vehicles and towers\n",
    "    edge_index = []\n",
    "    for vehicle_id in vehicle_ids:\n",
    "        for tower_id in tower_ids:\n",
    "            src = vehicle_mapping[vehicle_id]\n",
    "            dest = tower_mapping[tower_id]\n",
    "            edge_index.append([src, dest])\n",
    "    # Convert list to tensor\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "    # Example model invocation for the full set of potential edges\n",
    "def predict_all_edges(model, node_features, complete_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_edge_features = model(node_features, complete_edge_index)\n",
    "    return predicted_edge_features\n",
    "\n",
    "\n",
    "\n",
    "def map_predictions_to_ids(predicted_edge_features, edge_index, vehicle_mapping, tower_mapping):\n",
    "    mapped_predictions = {}\n",
    "    # Reverse the mappings to map index back to ID\n",
    "    index_to_vehicle = {v: k for k, v in vehicle_mapping.items()}\n",
    "    index_to_tower = {v: k for k, v in tower_mapping.items()}\n",
    "\n",
    "    # Loop through each edge and map the prediction to the corresponding vehicle and tower IDs\n",
    "    for i, (src, dest) in enumerate(edge_index.t().tolist()):\n",
    "        vehicle_id = index_to_vehicle.get(src, None)\n",
    "        tower_id = index_to_tower.get(dest - len(vehicle_mapping), None)  # Adjust index by the number of vehicles if necessary\n",
    "\n",
    "        if vehicle_id is not None and tower_id is not None:\n",
    "            mapped_predictions[(vehicle_id, tower_id)] = predicted_edge_features[i].tolist()\n",
    "\n",
    "    return mapped_predictions\n",
    "\n",
    "# Example execution within a testing loop for graphs\n",
    "# for graph_data in test_graphs:\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predicted_edge_features = model(graph_data.x, graph_data.edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "vehicle_mapping = {vid: i for i, vid in enumerate(vehicle_ids)}\n",
    "\n",
    "# Create mapping from tower ID to a unique index\n",
    "tower_mapping = {tid: i for i, tid in enumerate(tower_ids)}\n",
    "\n",
    "vehicle_ids = list(vehicle_mapping.keys())\n",
    "tower_ids = list(tower_mapping.keys())\n",
    "\n",
    "# Create a complete bipartite edge index\n",
    "complete_edge_index = create_complete_bipartite_edge_index(vehicle_ids, tower_ids, vehicle_mapping, tower_mapping)\n",
    "\n",
    "# Predict edge features for all possible edges\n",
    "predicted_edge_features = predict_all_edges(model, graph_data.x, complete_edge_index)\n",
    "\n",
    "# Map predictions back to vehicle and tower IDs\n",
    "mapped_predictions = map_predictions_to_ids(predicted_edge_features, complete_edge_index, vehicle_mapping, tower_mapping)\n",
    "print(mapped_predictions)\n",
    "# Print the mapped predictions\n",
    "for pair, features in mapped_predictions.items():\n",
    "    print(f\"Vehicle {pair[0]} and Tower {pair[1]} -> Predicted Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each vehicle select the best towers\n",
    "#return the estimated metrics for extended ho decision-making\n",
    "#check the number of edges\n",
    "\n",
    "#tune the metrics and history size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
