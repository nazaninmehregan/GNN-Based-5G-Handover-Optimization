{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGC model using torch geometric with Cora-Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import SGConv\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora:  Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "path = \"Cora\"\n",
    "dataset = Planetoid(path, \"Cora\")\n",
    "data = dataset[0] #tesnor representation of data\n",
    "print(\"Cora: \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type: Citation network\n",
    "Nodes: Papers (documents)\n",
    "Edges: Citations between papers\n",
    "Node Features: Word frequency vectors extracted from the text of each paper\n",
    "Labels: Each paper belongs to one of seven categories:\n",
    "Case_Based\n",
    "Genetic_Algorithms\n",
    "Neural_Networks\n",
    "Probabilistic_Methods\n",
    "Reinforcement_Learning\n",
    "Rule_Learning\n",
    "Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x represents the feature nodes.\n",
    "y represents the label of each node therefore it's just one vector.\n",
    "We have 2708 nodes, 1433 features for each node and 10556 edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 4, 4,  ..., 3, 3, 3])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "Number of classes: 7\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "print(data.y)\n",
    "unique_classes = torch.unique(data.y)\n",
    "print(unique_classes)\n",
    "# Get the number of classes\n",
    "num_classes = unique_classes.size(0)\n",
    "\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Number of classes:\", dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10556])\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTRUCT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGC_model = SGConv(in_channels=data.num_features, #number of features\n",
    "                   out_channels=dataset.num_classes, #dimension of embedding\n",
    "                   k=1, #number of avergaing iterations that we want\n",
    "                   cached=True # True if we want to use the same mathematical computation for all the different layers\n",
    "                   )\n",
    "\n",
    "#out_channel depends on the type of embedding you want to do and the dimensionality of embedded space \n",
    "# -> from 1433 dimensions to the number of classes is the optimal one\n",
    "#the optimal number of out channels in the case of classification is going to be the number of classes that each node is assigned to\n",
    "# here we have 7 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to take our data into an embedding space representation then we could easily do that by just passing our data into this model and then getting the output.\n",
    "\n",
    "The model is just doing an averaging over the feature nodes using edge data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET EMBEDDING\n",
    "\n",
    "We are just averaging over the data features using the edge information so the raw embedding model that we get has the following form (we are skipping the training step)\n",
    "\n",
    "We are using the SGC_model to perform the forward pass on the input features (data.x) and edge information (data.edge_index), obtaining node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original data:  torch.Size([2708, 1433])\n",
      "Shape of the raw embedding data:  torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the original data: \", data.x.shape)\n",
    "print(\"Shape of the raw embedding data: \", SGC_model(data.x, data.edge_index).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTRUCT THE MODEL AGAIN FOR NODE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the same model however we want to optimize the parameters such as weight and bias so that we can get a good classification result.\n",
    "\n",
    "We are going to repeat the construction of the model but this time we are going to say it's going to be for classification.\n",
    "\n",
    "Here we are contructing the SCG as a neural network model so we could add more layers, do a back propagation on the parameters given a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels=dataset.num_features,\n",
    "            out_channels=dataset.num_classes,\n",
    "            K=2,\n",
    "            cached=True,\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.conv1(data.x, data.edge_index) # applying convoulution to data\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INIT: What we need to do when constructing a class of nn in a pytorch rep is to define specific layers in init function and then use a forward method to define how these specific layers have to be constructed.\n",
    "\n",
    "FORWARD: forward model is this part of the embedding that we have -> SGC_model(data.x, data.edge_index)\n",
    "so we are just passing the data which is already known with its features and edges into our model which is conv1 and we are getting the output.\n",
    "\n",
    "OUTPUT: the output is just a logarthimic softmax. This is because we are constructing this class specifically for classification. -> Output is 7 or whatever the number of classes is and we want this to be a probability. We want to use this output in order to get a prabability of a specific label that's why we are setting it as the number of the classes. \n",
    "If we just wanted an embedding represenation, we could define the output based on whatever number of embeddings we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SGC_model, data = SGCNet().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(SGC_model.parameters(), lr=0.2, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we don't need to introduce a loader and batch optimization because the simplified graph convolution is very fast compared to Node2Vec which is a probabilistic approach and requires an iterative function where as the SCG is just a simple algebra multiplication in order to get an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the Learning Parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter conv1.lin.weight\n",
      "Shape:  torch.Size([7, 1433])\n",
      "Parameter conv1.lin.bias\n",
      "Shape:  torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "#here at first you had SGC_model and the output was juct lin.weight but after adding SGCNet() as your model it became conv1.lin.weight\n",
    "for i, parameter in SGC_model.named_parameters():\n",
    "    print(\"Parameter {}\".format(i))\n",
    "    print(\"Shape: \", parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what we do in this function?\n",
    "1. get the data, \n",
    "2. pass the data onto the model that we have constructed, \n",
    "3. get the corresponding probability which is technically output\n",
    "4. compare those outputs with our true labels\n",
    "5. optimize the model based on those true labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    SGC_model.train() # set the model.training to be true\n",
    "    optimizer.zero_grad() # reset the gradient\n",
    "    predicted_y = SGC_model() # our result from the model when it's beeen trained in log softmax prob\n",
    "    true_y = data.y # true labels\n",
    "    # find the error differnce between true y and predicted y based on the training data\n",
    "    # F.nll_loss(SGC_model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    losses = F.nll_loss(predicted_y[data.train_mask], true_y[data.train_mask])\n",
    "    losses.backward() # backward propagation\n",
    "    optimizer.step() # update the parameters such that it minimizes the losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    SGC_model.eval() # set the model.training to be false\n",
    "    logits = SGC_model() # compute log probability of all data\n",
    "    accs = []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1] #transforming log probability to actual labels so instead of having 0.2 which is prob we predict the number of class e.x. 3\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUTTING IT ALL TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.9500, Val: 0.6180, Test: 0.6440\n",
      "Epoch: 002, Train: 0.9500, Val: 0.7580, Test: 0.7790\n",
      "Epoch: 003, Train: 0.9786, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 004, Train: 0.9857, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 005, Train: 0.9714, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 006, Train: 0.9786, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 007, Train: 1.0000, Val: 0.7660, Test: 0.7790\n",
      "Epoch: 008, Train: 1.0000, Val: 0.7780, Test: 0.8110\n",
      "Epoch: 009, Train: 1.0000, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 010, Train: 0.9929, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 011, Train: 0.9857, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 012, Train: 0.9857, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 013, Train: 0.9929, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 014, Train: 1.0000, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 015, Train: 1.0000, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 016, Train: 1.0000, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 017, Train: 0.9929, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 018, Train: 0.9929, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 019, Train: 1.0000, Val: 0.7900, Test: 0.8220\n",
      "Epoch: 020, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 021, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 022, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 023, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 024, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 025, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 026, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 027, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 028, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 029, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 030, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 031, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 032, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 033, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 034, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 035, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 036, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 037, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 038, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 039, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 040, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 041, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 042, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 043, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 044, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 045, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 046, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 047, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 048, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 049, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 050, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 051, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 052, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 053, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 054, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 055, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 056, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 057, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 058, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 059, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 060, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 061, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 062, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 063, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 064, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 065, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 066, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 067, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 068, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 069, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 070, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 071, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 072, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 073, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 074, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 075, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 076, Train: 1.0000, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 077, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 078, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 079, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 080, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 081, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 082, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 083, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 084, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 085, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 086, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 087, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 088, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 089, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 090, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 091, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 092, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 093, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 094, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 095, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 096, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 097, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 098, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 099, Train: 0.9929, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 100, Train: 0.9929, Val: 0.7960, Test: 0.8230\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUCH FASTER!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more comprehensive implementation of:\n",
    "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sgc.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
